{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short video we're going to talk about a really common task in applied data science, which is the\n",
    "resampling of time data. This is often done when you have say, intermittent data which you want to make more\n",
    "regular, or when you have really fine grained data and you want to get more general trends of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling and Frequency Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling refers to the process of converting a time series from one frequency to another. Converting from\n",
    "# a higher frequency to a lower frequency is called downsampling and the other way around is called\n",
    "# upsampling. There is another type of sampling, which is neither upsampling or downsampling such asbchanging a\n",
    "# weekly Wednesday value into a weekly Sunday value.\n",
    "\n",
    "# Resampling is useful and commonly used in manipulating time related data. Now let's see how it is done in\n",
    "# Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create some datimeindex using date_range() function. We can set either start or end, and\n",
    "# specify the frequency and number of periods. Here we set date as Jan 1st 2018 and end as May 31st 2018 and\n",
    "# use the dates in creating a series with random numbers\n",
    "dates = pd.date_range(start='1/1/2018', end='05/31/2018')\n",
    "ts = pd.Series(np.random.randn(len(dates)), index=dates)\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try downsampling. Two important things need to be considered when downsampling. First, which side\n",
    "# of each interval is closed. Let's say we want to converting daily frequency to weekly frequency. You need to\n",
    "# chop up the data into one week intervals. Each interval is said to be half-open. A data point can only\n",
    "# belong to one interval and the union of the intervals must make up the whole time frame.\n",
    "\n",
    "# Secondly, we need to decide how the new aggregated bins should be labeled. Either from the start of the\n",
    "# interval or the end of the interval.\n",
    "\n",
    "# Let's look at an example using the Series we just created. Here we want to convert daily to weekly. We can\n",
    "# use the resample() function. The resample function has parameters for specifying the new frequency, which\n",
    "# side is closed. After that, we also have to decide what kind of aggreate function we want to do with each\n",
    "# interval.\n",
    "\n",
    "# Here, we specify the frequency as weekly, which is W, and the closed side is right, aggregate function is\n",
    "# mean\n",
    "ts.resample('W', closed='right').mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just take a look under the hood, what is this object returned by resample()?\n",
    "type(ts.resample('W', closed='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This object allows us to resample pretty much however we want, through use of the agg() function, but it\n",
    "# also holds many of the common functions we might use, such as mean(). For instance, if we just wanted to\n",
    "# count all of the data values that were being resampled, we could use len() and write our own lambda\n",
    "ts.resample('W', closed='right').agg(lambda x: len(x)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we pay attention to the bottom of the output, where it says the frequency is \"W-SUN\", it means weekly on\n",
    "# Sunday. If we want to do another day, for instance, Wednesday, we can do \"W-WED\"\n",
    "\n",
    "# After converting the frequency, Pandas also allows us to adjust the labels with the loffset parameter. If we\n",
    "# change the daily frequency to monthly frequency, and set the loffset to -1d, which is a month backward,\n",
    "# let's see and example.\n",
    "ts.resample('M', closed='right', label='right', loffset='-1M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another popular and useful approach to aggregate is to compute four values for each bucket: the first, last,\n",
    "# maximum, and minimal values. By using the ohlc() function, we can get a dataframe with the new frequency\n",
    "# indices and four columns containing the four values at each time period\n",
    "ts.resample('M', closed='right', label='right', loffset='-1M').ohlc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a pretty common financial data routine, as you might guess from the names of the columns, but you\n",
    "# can write your own functions and pass them to agg() or apply() as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's talk about upsampling. Since we are converting lower frequency to higher frequency, there is no\n",
    "# need to aggregate. We use the asfreq() function to convert to the higher frequency without any aggregation\n",
    "\n",
    "# let's create a dataframe, with two weekly indices, and four columns. First the indicies\n",
    "dates = pd.date_range(start='1/1/2018', periods=2, freq='W')\n",
    "# now let's fill in the DataFrame\n",
    "df = pd.DataFrame(np.random.randn(2,4), index=dates, \n",
    "                  columns=['col1','col2','col3','col4'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we upsample from weekly frequency to daily frequency,\n",
    "# we use the resample() function with frequency to \"D\" and the asfreq() function\n",
    "df_daily = df.resample('D').asfreq()\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you notice, there will be NaN values in some cells because we are upsampling and we do not have the data\n",
    "# for some of the new intervals. If you want to fill the NaN values, which is called interpolation, you can\n",
    "# either use ffill(), which we have talked about and is forward filling; or bfill() which is backward filling.\n",
    "# Or you can use fillna() or reindex() methods.\n",
    "\n",
    "# In our dataframe, it makes sense to do forward filling, now let's try it\n",
    "df.resample('D').ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also choose to only fill a certain number of periods, by using the limit parameter in the ffill()\n",
    "# function. For instance, here, we are limiting to interpolating three observations\n",
    "df.resample('D').ffill(limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An important group of manipulation techniques on time series are focused on over a sliding window or with\n",
    "# exponentially decaying weights. This is very useful for smoothing noisy or gappy data. Note that these kinds\n",
    "# of functions automatically exclude missing data.\n",
    "\n",
    "# Now let's look at examples on the stock market. We are going to look at Apple and Microsoft's daily stock\n",
    "# price from 2012 to 2018\n",
    "apple = pd.read_csv(\"datasets/AAPL.csv\")\n",
    "ms = pd.read_csv(\"datasets/MSFT.csv\")\n",
    "\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we see here, we have different kinds of pricing. For the analysis we are going to do, we will use close\n",
    "# price. Let's combine Apple and Microsoft's daily prices together into one dataframe.\n",
    "df = pd.DataFrame({'AAPL': apple['Close'],'MSFT':ms['Close']})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the prices over the years\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to learn the rolling operator, which behaves similarly to resample and groupby. It can be\n",
    "# called on a Series or a Dataframe along with a window, which is a number of periods to cover. The number we\n",
    "# specify in rolling() function means the sliding window we are group by. For example, if we do 100, that is\n",
    "# grouping over a 100-day sliding window\n",
    "\n",
    "# Here, let's do a 100 day rolling window where we average the values and plot it.\n",
    "df.rolling(100).mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that this not only smoothed the data, but we lost the big dropped at the end of the time period\n",
    "# for apple because of the size of our window. Try playing with a few window values yourself and get a sense\n",
    "# for how that might change the insights you derive from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've just touched the very basics of manipulating time series data in python. These techniques will be\n",
    "useful for conducting further time series analysis and for more advanced data visualization on time related\n",
    "data, as well as when dealing with feature engineering from time series sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
