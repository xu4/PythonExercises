{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture might seem a bit backwards to some - you've already learned how to manipulate dataframes\n",
    "loading data from structured formats such as CSVs. But there are many more common data formats that pandas\n",
    "can help you with, and I think it's important to have a bit more of a comprehensive look at data storage, so\n",
    "here we're going to expand our investigation to other data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the pandas library \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV & Type Inference \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the most common and easiest files to work with is a CSV file or comma-separated text file. Loading,\n",
    "# reading and writing CVS files is a key skill to master for any data scientist.\n",
    "\n",
    "# To review,a  CSV is a txt file where values or columns are separated by commas CSV is quick to read and is\n",
    "# compatible with most platforms, you can even open a CSV file with the text editor built into Jupyter\n",
    "\n",
    "# While CSV is very easy to load, real world data rarly contains clean data in these cases, there are a few\n",
    "# different pandas functions that can help out with messy data here is an example of a CSV file I pulled from\n",
    "# google containing data on national education.\n",
    "df= pd.read_csv ('datasets/National-2018-20190321.csv')\n",
    "print(df.head())\n",
    "\n",
    "# We can summerize the counts of unique values for a specific collumn, so the most frequently-occurring\n",
    "# element and how much they occur\n",
    "df['Qualification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you load a csv file, python knows that the values will be separated by commas, but CSV files donâ€™t\n",
    "# contain any type information for the data, so Pandas infers the data types unless you specify the value type\n",
    "# for columns with the dtype parameter. We can quickly see the datatype infered by pandas using the DataFrame\n",
    "# .dtypes attribute\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually the first row in a CSV file contain the names of the columns for the data, and when it doesn't\n",
    "# you can either specify columns yourself or let pandas insert an autoincrementing number instead. A handy\n",
    "# option is to actually set the index of the DataFrame to be one of the columns as well, on load.\n",
    "pd.read_csv('datasets/National-2018-20190321.csv',index_col=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also insert a multiindex by just providing a list, and you can use column names as infered from the\n",
    "# first row.\n",
    "pd.read_csv('datasets/National-2018-20190321.csv', index_col=['Year Level', 'Academic Year']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes a formatting issue can occur preventing you from working with your data at all. While you should\n",
    "# always ensure you are working with the data you want to, it is sometimes useful to skip troublesome rows\n",
    "# for data exploration\n",
    "pd.read_csv('datasets/National-2018-20190321.csv', skiprows=[1, 2, 3]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes you are dealing with large CSVs and reading them in one operation is intractable. To help deal\n",
    "# with this you can give pandas a \"chunksize\", then iterate over dataframes in a chunk by chunk manner\n",
    "chunker = pd.read_csv('datasets/National-2018-20190321.csv', chunksize=13)\n",
    "i=0\n",
    "for piece in chunker:\n",
    "    print(\"Chunk {} has {} rows\".format(i, len(piece)))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use this when I have a fairly extensive data cleaning or processing routine that I want to run on large\n",
    "# datafiles to both reduce overall memory usage as well as improve CPU utilization. Now, this isn't\n",
    "# particularly important for this class, but as you progress as a data scientist there will come a time when\n",
    "# you need to load and work with bigish data - data that you should be able to work with on a laptop but can't\n",
    "# - and knowing that there are places to investigate further is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something you should be aware with chunking, however, is that the pandas type inference works on a chunk-by-\n",
    "# chunk size. So your types might end up being a bit unpredictictable. In the above example there is one cell\n",
    "# in the \"Academic Year\" column which has a non-number. When we read in the whole dataframe this is read as\n",
    "# a type of object. But when we read in by chunks we see this is only an object for one chunk.\n",
    "for piece in pd.read_csv('datasets/National-2018-20190321.csv', chunksize=13):\n",
    "    print(piece[\"Academic Year\"].dtype)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps even more ubiquitous than CSV files in the corporate world are Excel files. You'll find that much\n",
    "# of the data created or given to you might be trapped in this format. Using the excellent xlrd library,\n",
    "# pandas is able to read this almost as seamlessly as CSV files.\n",
    "\n",
    "# An important difference from CSV files is that a single Excel workbook might contain many sheets, each\n",
    "# analagous to a DataFrame. Lets load some data from a report card on Elementary Districts from 2016-17\n",
    "# and here I'll just look at one sheet, the Primary Class Sizes\n",
    "pd.read_excel('datasets/classsizes.xlsx',sheet_name='Primary Class Sizes').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hrm. That table doesn't look so useful, let's take a look at what this looks like in Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Excel](excel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so this is a pretty typical data file you might be handed to be able to work with. We see a number\n",
    "# of different rows being taken up with graphics and the like, colors being used for headers, and that there\n",
    "# are at least two DataFrames on this page. To deal with this you're going to want to use a number of the\n",
    "# different options in the read_excel() function.\n",
    "\n",
    "# Excel numbers its rows starting at 1, but we don't, we start at 0. And Excel has columns starting with A,\n",
    "# but with pandas we use numbers instead of letters, again starting at 0. Lets grab that top table into a df\n",
    "df=pd.read_excel('datasets/classsizes.xlsx',sheet_name='Primary Class Sizes',\n",
    "              header=7,skipfooter=19,index_col=0,dtype={'Average class size': str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The easiest way to store data in binary format is using python's built-in pickle serialization. Now, this\n",
    "# isn't a pandas library, this is built into the language. It's quick and easy to dump an object, like a\n",
    "# DataFrame, to a file to use it later. There are lots of caveats to doing this and I wouldn't recommend it\n",
    "# over some of the other formats we'll talk about, but if you're working mostly with Python developers you\n",
    "# will undoubtedly need to use it.\n",
    "#\n",
    "# And, one of the most useful parts of pickle versus some of the other formats we have talked about is that\n",
    "# datatypes are preserved. So you can do all of your cleaning and typing in one file, then share the object\n",
    "# with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets import pickle \n",
    "import pickle\n",
    "\n",
    "# Now we are going to create the pickle file we want to write to\n",
    "with open('dataframe.pickle', 'wb') as f:\n",
    "    # Now we just tell python to dump our dataframe from the previous example into this pickle file\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And when we want to read it in again we can do so easily\n",
    "with open('dataframe.pickle', 'rb') as f:\n",
    "    our_old_dataframe = pickle.load(f)\n",
    "our_old_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle files are a quick and easy way to store DataFrames, and I find that sometimes I use them when\n",
    "# I need to save intermediate DataFrames, for instance when I'm working with large dataframes on a machine\n",
    "# with a limited amount of memory, or when I need to distribute portions of a dataframe to different\n",
    "# processes or machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDF5 is used when you're working with very large datasets, and it is especially useful for datasets that \n",
    "# wont fit into memory. \"HDF\" stands for hierarchical data format, and each HDF5 file can store multiple \n",
    "# datasets as well as supporting metadata (like column information). HDF5 supports on-the-fly compression \n",
    "# with a variety of compression modes, enabling data with repeated patterns to be stored more efficiently \n",
    "\n",
    "# Let's bring in the numpy library\n",
    "import numpy as np\n",
    "\n",
    "# To use HDF5 we first create our data store as a file\n",
    "store = pd.HDFStore('mydata.h5')\n",
    "\n",
    "# Then we can insert into that store different DataFrames or even portions of frames\n",
    "store['class_sizes']=df\n",
    "store['class_size_2009']=df[2009]\n",
    "\n",
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see what keys are available in the HDFStore with .keys()\n",
    "store.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects contained in the HDF5 file can then be retrieved with the same dict-like API \n",
    "store['class_sizes'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But where HDF5 really shines is when you save  an object of type \"table\"\n",
    "store.put('a_new_class_size', df, format='table')\n",
    "store.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you do this, you can retrieve portions of your DataFrame from disk without having to read the whole\n",
    "# DataFrame into memory\n",
    "store.select('a_new_class_size', where=['index=Prep'], columns=[2009,2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A deeper investigation of HDF5 is outside of the scope of this lecture, but I would encourage you to check\n",
    "# out the online docs and play a bit with it, especially the novel heirarchical nature of the data. Where it\n",
    "# is used most commonly is large scientific datasets which are collected once but read many times. A more\n",
    "# common alternative, which we won't cover in this course due to it's size and complexity, are relational\n",
    "# databases, and there are several ways of interacting with these data sources from within python and pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyarrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last datafile format I'll talk about here is Pyarrow and Feather format. The Feather format is an\n",
    "# open-source columnar storage format - so a DataFrame format - which intended to be a replacement for CSV\n",
    "# files and is both faster to read and write as well as preserves metadata about columns.\n",
    "\n",
    "# Let's do some time comparisons reading in the same dataframe from both CSV and Feather formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "# First the CSV reading\n",
    "df_csv=pd.read_csv(\"datasets/house_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "# Now the feather reading\n",
    "df_feather=pd.read_feather(\"datasets/house_prices.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we see that the feather reading is moderatly faster. Lets look at the dataframe types and compare\n",
    "# them\n",
    "df_csv=pd.read_csv(\"datasets/house_prices.csv\")\n",
    "print(df_csv.dtypes)\n",
    "df_feather=pd.read_feather(\"datasets/house_prices.feather\")\n",
    "print(df_feather.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One difference we can see is that the sale_date field is correctly set to datetime64 in the feather\n",
    "# DataFrame, but was just interpreted as a generic object in the CSV dataframe. Now, we could probably further\n",
    "# tweak some of these datatypes to increase speed a little bit if we wanted to, dropping the size of the\n",
    "# integer and float values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But I don't want to give you the thought you should pivot and start using feather for everything. It's very\n",
    "# much under active development, and there are some potential catches or defaults that you might not expect.\n",
    "# For instance, I thought preserving a multi-index would work, but it doesn't\n",
    "df_feather=df_feather.set_index([\"state\",\"city\"])\n",
    "df_feather.to_feather(\"house_prices.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default behavior for CSVs is different, and the to_csv() function writes the columns (though not as a\n",
    "# multi-index, just as regular columns). Regardless, feather is a great format to watch evolve, and the \n",
    "# bindings across programming languages means there is potential for high interoperability and high\n",
    "# performance of DataFrame data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we covered a range of datafile formats for DataFrames. By far the most common is the CSV,\n",
    "but it's useful to know that there are other formats you might have to work with, and that pandas supports\n",
    "them. Excel files are very common in large organizations, pickle files are quick to move between python\n",
    "processes and capture all the nuance of the DataFrame object, and HDF5 and Feather files have their uses in\n",
    "more specialized circumstances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
